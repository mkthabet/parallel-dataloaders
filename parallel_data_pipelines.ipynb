{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"MzK8cHpy9uoi"},"outputs":[],"source":["import torch\n","import tensorflow as tf\n","import time\n","import multiprocessing as mp"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sch9t24MJD_M"},"outputs":[],"source":["batch_size = 32\n","dataset_length = 32*10\n","cpu_count = mp.cpu_count()\n","print(f'CPUs available: {cpu_count}')\n","\n","def do_work():\n","    for i in range(1_000_000):\n","        i = i + 1\n","\n","def do_sleep():\n","    time.sleep(0.05)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WJz1ar1bB9-O"},"outputs":[],"source":["class TFDataLoaderThin():\n","    def gen_sample(self):\n","        for idx in range(dataset_length):\n","            yield idx # dummy data\n","\n","def map_fn_io(x):\n","    tf.py_function(do_sleep, [], ())\n","    return x\n","    \n","def map_fn_cpu(x):\n","    tf.py_function(do_work, [], ())\n","    return x"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mloF02eCJD_P"},"outputs":[],"source":["dataloader = TFDataLoaderThin()\n","datagen = dataloader.gen_sample\n","tf_dataset = tf.data.Dataset.from_generator(datagen, output_signature=tf.TensorSpec([], tf.int32))\n","tf_dataset = tf_dataset.map(map_fn_io).batch(batch_size)\n","# time the dataloader\n","start = time.time()\n","for idx, data in enumerate(tf_dataset):\n","    pass\n","print(\"Time taken for TF vanilla IO dataset: {}\".format(time.time() - start))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wTvMhITlM78T"},"outputs":[],"source":["dataloader = TFDataLoaderThin()\n","datagen = dataloader.gen_sample\n","tf_dataset = tf.data.Dataset.from_generator(datagen, output_signature=tf.TensorSpec([], tf.int32))\n","tf_dataset = tf_dataset.map(map_fn_io, num_parallel_calls=cpu_count).batch(batch_size)\n","# time the dataloader\n","start = time.time()\n","for idx, data in enumerate(tf_dataset):\n","    pass\n","print(\"Time taken for TF parallel IO dataset: {}\".format(time.time() - start))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rioOn7HbNBsx"},"outputs":[],"source":["dataloader = TFDataLoaderThin()\n","datagen = dataloader.gen_sample\n","tf_dataset = tf.data.Dataset.from_generator(datagen, output_signature=tf.TensorSpec([], tf.int32))\n","tf_dataset = tf_dataset.map(map_fn_cpu).batch(batch_size)\n","# time the dataloader\n","start = time.time()\n","for idx, data in enumerate(tf_dataset):\n","    pass\n","print(\"Time taken for TF vanilla CPU dataset: {}\".format(time.time() - start))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0jY5zAHKNhGN"},"outputs":[],"source":["dataloader = TFDataLoaderThin()\n","datagen = dataloader.gen_sample\n","tf_dataset = tf.data.Dataset.from_generator(datagen, output_signature=tf.TensorSpec([], tf.int32))\n","tf_dataset = tf_dataset.map(map_fn_cpu, num_parallel_calls=cpu_count).batch(batch_size)\n","# time the dataloader\n","start = time.time()\n","for idx, data in enumerate(tf_dataset):\n","    pass\n","print(\"Time taken for TF parallel CPU dataset: {}\".format(time.time() - start))"]},{"cell_type":"code","source":["class TFDataLoader():\n","    def __init__(self, num_workers=1):\n","        self.num_workers = num_workers\n","        self.queue = mp.Queue(maxsize=10)\n","        self.processes = []\n","        self.chunk_size = dataset_length//self.num_workers # split the dataset into chunks here\n","\n","    def initialize(self):\n","        processes = []\n","        for i in range(self.num_workers):\n","            p = mp.Process(target=self._worker, daemon=True)\n","            processes.append(p)\n","        for p in processes:\n","            p.start()\n","        self.processes = processes\n","\n","    def _worker(self):\n","        for idx in range(self.chunk_size):\n","            do_work()\n","            self.queue.put(idx)\n","\n","    def gen_sample(self):\n","        processed = 0\n","        while processed < dataset_length:\n","            yield self.queue.get()\n","            processed += 1\n","\n","    def close(self):\n","        for p in self.processes:\n","            p.terminate()"],"metadata":{"id":"5UBflFKlUfFJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["    dataloader = TFDataLoader(num_workers=1)\n","    dataloader.initialize()\n","    datagen = dataloader.gen_sample\n","    tf_dataset = tf.data.Dataset.from_generator(datagen, output_signature=tf.TensorSpec([], tf.int32))\n","    tf_dataset = tf_dataset.batch(batch_size)\n","    # time the dataloader\n","    start = time.time()\n","    for idx, data in enumerate(tf_dataset):\n","        pass\n","    print(\"Time taken for TF vanilla CPU dataset: {}\".format(time.time() - start))\n","    dataloader.close()"],"metadata":{"id":"L2D5MjGSUhUr"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["    dataloader = TFDataLoader(num_workers=cpu_count)\n","    dataloader.initialize()\n","    datagen = dataloader.gen_sample\n","    tf_dataset = tf.data.Dataset.from_generator(datagen, output_signature=tf.TensorSpec([], tf.int32))\n","    tf_dataset = tf_dataset.batch(batch_size)\n","    # time the dataloader\n","    start = time.time()\n","    for idx, data in enumerate(tf_dataset):\n","        pass\n","    print(\"Time taken for TF parallel CPU dataset: {}\".format(time.time() - start))\n","    dataloader.close()"],"metadata":{"id":"O5K_JuJmVNow"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8F52Yjc3_Bpm"},"outputs":[],"source":["class CPUDataset(torch.utils.data.Dataset):\n","    def __len__(self):\n","        return dataset_length\n","\n","    def __getitem__(self, idx):\n","        do_work()\n","        return idx\n","    \n","class IODataset(torch.utils.data.Dataset):\n","    def __len__(self):\n","        return dataset_length\n","\n","    def __getitem__(self, idx):\n","        do_sleep()\n","        return idx"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0YFbN8nv_q7e"},"outputs":[],"source":["cpu_dataset = CPUDataset()\n","dataloader = torch.utils.data.DataLoader(cpu_dataset, batch_size=batch_size)\n","# time the dataloader\n","start = time.time()\n","for idx, data in enumerate(dataloader):\n","    pass\n","print(\"Time taken for torch vanilla CPU dataset: {}\".format(time.time() - start))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NrnNg2rgJD_N"},"outputs":[],"source":["cpu_dataset = CPUDataset()\n","dataloader = torch.utils.data.DataLoader(cpu_dataset, batch_size=batch_size, num_workers=cpu_count)\n","# time the dataloader\n","start = time.time()\n","for idx, data in enumerate(dataloader):\n","    pass\n","print(\"Time taken for torch parallel CPU dataset: {}\".format(time.time() - start))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5L8obGtBN_os"},"outputs":[],"source":["io_dataset = IODataset()\n","dataloader = torch.utils.data.DataLoader(io_dataset, batch_size=batch_size)\n","# time the dataloader\n","start = time.time()\n","for idx, data in enumerate(dataloader):\n","    pass\n","print(\"Time taken for torch vanilla IO dataset: {}\".format(time.time() - start))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"J0Vzl_PfN_5L"},"outputs":[],"source":["io_dataset = IODataset()\n","dataloader = torch.utils.data.DataLoader(io_dataset, batch_size=batch_size, num_workers=cpu_count)\n","# time the dataloader\n","start = time.time()\n","for idx, data in enumerate(dataloader):\n","    pass\n","print(\"Time taken for torch parallel IO dataset: {}\".format(time.time() - start))"]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"medical_env","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.12"},"vscode":{"interpreter":{"hash":"575e40928c11cd0238fd6bc914f6b16dd1c4601e7697ba6b3aa0da44f5f3f5bd"}}},"nbformat":4,"nbformat_minor":0}